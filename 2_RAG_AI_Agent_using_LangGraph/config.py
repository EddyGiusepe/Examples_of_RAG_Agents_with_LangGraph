#!/usr/bin/env python3
"""
Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro

Configura√ß√µes do Projeto RAG Agent com LangGraph
=================================================
Este m√≥dulo centraliza todas as configura√ß√µes do projeto, incluindo:
- Carregamento de vari√°veis de ambiente
- Configura√ß√µes do modelo LLM
- Configura√ß√µes de embeddings
- Par√¢metros do banco vetorial
- Configura√ß√µes de chunking de documentos
"""

import os
from pathlib import Path
from dotenv import load_dotenv, find_dotenv

# ============================================================================
# Carregamento de Vari√°veis de Ambiente
# ============================================================================

# Tenta encontrar e carregar o arquivo .env
_ = load_dotenv(find_dotenv())

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

# Valida√ß√£o b√°sica
if not OPENAI_API_KEY:
    raise ValueError(
        "‚ùå OPENAI_API_KEY n√£o encontrada!\n"
        "Por favor, crie um arquivo .env com sua chave da OpenAI.\n"
        "Veja o arquivo .env.example para refer√™ncia."
    )

# ============================================================================
# Configura√ß√µes do Projeto
# ============================================================================

# Diret√≥rio raiz do projeto (2_RAG_AI_Agent_using_LangGraph/)
PROJECT_ROOT = Path(__file__).parent

# Diret√≥rio para armazenar dados
DATA_DIR = PROJECT_ROOT / "data"
DATA_DIR.mkdir(exist_ok=True)

# Diret√≥rio para o banco vetorial
VECTOR_DB_DIR = PROJECT_ROOT / "chroma_db"

# ============================================================================
# Configura√ß√µes do Modelo LLM
# ============================================================================

# Modelo da OpenAI a ser usado
LLM_MODEL = "gpt-5-nano"  # Mais r√°pido e econ√¥mico
# LLM_MODEL = "gpt-4o"  # Mais poderoso, mas mais caro

# Temperatura do modelo (0.0 = mais determin√≠stico, 1.0 = mais criativo)
LLM_TEMPERATURE = 0.0

# N√∫mero m√°ximo de tokens na resposta
LLM_MAX_TOKENS = 1000

# ============================================================================
# Configura√ß√µes de Embeddings
# ============================================================================

# Modelo de embeddings da OpenAI
EMBEDDING_MODEL = "text-embedding-3-small"  # Mais r√°pido e barato
# EMBEDDING_MODEL = "text-embedding-3-large"  # Maior qualidade

# Dimens√£o dos embeddings (apenas para text-embedding-3-*)
# Valores menores s√£o mais r√°pidos, valores maiores t√™m melhor qualidade
EMBEDDING_DIMENSION = 1536  # Padr√£o do text-embedding-3-small

# ============================================================================
# Configura√ß√µes do Vector Store (Chroma)
# ============================================================================

# Nome da cole√ß√£o no Chroma
COLLECTION_NAME = "rag_documents"

# N√∫mero de documentos a retornar em cada busca
RETRIEVAL_K = 4  # Top 4 documentos mais relevantes

# Tipo de busca: "similarity" (padr√£o) ou "mmr" (Maximum Marginal Relevance)
SEARCH_TYPE = "similarity"

# ============================================================================
# Configura√ß√µes de Chunking (Divis√£o de Documentos)
# ============================================================================

# Tamanho de cada chunk em caracteres
CHUNK_SIZE = 1000

# Overlap entre chunks (para manter contexto)
CHUNK_OVERLAP = 200

# ============================================================================
# Configura√ß√µes do Agente
# ============================================================================

# System prompt para o agente
SYSTEM_PROMPT = """Voc√™ √© um assistente especializado em responder perguntas com base em
                   documentos fornecidos.

INSTRU√á√ïES:
1. Use a ferramenta 'retrieve_context' para buscar informa√ß√µes relevantes nos documentos
2. Base suas respostas APENAS nas informa√ß√µes encontradas nos documentos
3. Se n√£o encontrar informa√ß√£o relevante nos documentos, diga que n√£o tem informa√ß√£o suficiente
4. Seja claro, conciso e objetivo nas respostas
5. Cite trechos dos documentos quando apropriado

Sempre priorize a precis√£o sobre a criatividade."""

# M√°ximo de itera√ß√µes do agente (evita loops infinitos)
MAX_ITERATIONS = 3

# ============================================================================
# Tipos de Arquivos Suportados
# ============================================================================

SUPPORTED_EXTENSIONS = {
    ".txt": "text",
    ".pdf": "pdf",
    ".md": "markdown",
    ".json": "json",
}

# ============================================================================
# Configura√ß√µes de Logging
# ============================================================================

# N√≠vel de logging
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Exibir mensagens de progresso
VERBOSE = True

# ============================================================================
# Fun√ß√µes Auxiliares
# ============================================================================

def print_config():
    """Exibe as configura√ß√µes atuais do projeto"""
    print("\n" + "="*60)
    print("‚öôÔ∏è  CONFIGURA√á√ïES DO PROJETO")
    print("="*60)
    print(f"üìÅ Diret√≥rio de dados: {DATA_DIR}")
    print(f"üóÑÔ∏è  Banco vetorial: {VECTOR_DB_DIR}")
    print(f"ü§ñ Modelo LLM: {LLM_MODEL}")
    print(f"üìä Modelo Embeddings: {EMBEDDING_MODEL}")
    print(f"üìë Tamanho do chunk: {CHUNK_SIZE} caracteres")
    print(f"üîÑ Overlap: {CHUNK_OVERLAP} caracteres")
    print(f"üîç Top-K retrieval: {RETRIEVAL_K}")
    print("="*60 + "\n")


def validate_config():
    """Valida se todas as configura√ß√µes necess√°rias est√£o presentes"""
    errors = []
    
    if not OPENAI_API_KEY:
        errors.append("OPENAI_API_KEY n√£o configurada")
    
    if CHUNK_SIZE <= CHUNK_OVERLAP:
        errors.append("CHUNK_SIZE deve ser maior que CHUNK_OVERLAP")
    
    if RETRIEVAL_K < 1:
        errors.append("RETRIEVAL_K deve ser pelo menos 1")
    
    if errors:
        raise ValueError(
            "‚ùå Erros de configura√ß√£o encontrados:\n" + 
            "\n".join(f"  - {error}" for error in errors)
        )
    
    return True


# Executa valida√ß√£o ao importar
try:
    validate_config()
except ValueError as e:
    print(f"\n{e}\n")
    raise


# ============================================================================
# Uso
# ============================================================================

if __name__ == "__main__":
    # Exibe as configura√ß√µes quando executado diretamente
    print_config()
    print("‚úÖ Configura√ß√µes validadas com sucesso!")

